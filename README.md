# FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention

<a href="https://arxiv.org/abs/2505.21144"><img src="https://img.shields.io/badge/arXiv-2505.21144-b31b1b.svg" height=22.5><a>
<a href="https://colab.research.google.com/drive/1-Bs5YHwi-dJH0gtU4y0DTY-oLKbhMYkE?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=22.5></a>
[![License](https://img.shields.io/github/license/AIRI-Institute/al_toolbox)](./LICENSE)

> This is a development fork of [ControlGenAI/FastFace](https://github.com/ControlGenAI/FastFace) with additional features for the Prometheus project.

**Added Features:**
- Mac MPS (Apple Silicon) support
- Web-based UI (React frontend + FastAPI backend)
- Style image transfer (img2img approach)

---

**Original Description:**
>In latest years plethora of identity-preserving adapters for a personalized generation with diffusion models have been released. Their main disadvantage is that they are dominantly trained jointly with base diffusion models, which suffer from slow multi-step inference. This work aims to tackle the challenge of training-free adaptation of pretrained ID-adapters to diffusion models accelerated via distillation - through careful re-design of classifier-free guidance for few-step stylistic generation and attention manipulation mechanisms in decoupled blocks to improve identity similarity and fidelity, we propose universal FastFace framework. Additionally, we develop a disentangled public evaluation protocol for id-preserving adapters.

![image](docs/method_scheme_promo.png)

## Updates

- **2025/12/05** Mac MPS support, Web UI, Style image transfer
- **2025/6/04** colab demo released
- **2025/5/28** code and data release
- **2025/5/27** arxiv preprint release


## Installation

### CUDA (Linux/Windows)

Clone repository, `cd` to directory and run following commands:

```bash
python -m venv env
source activate env/bin/activate
pip install -r requirements.txt
pip install onnxruntime-gpu==1.18.0 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/
```

**Important:** depending on underlying versions of `cudnn` and `cuda` backend you might need to tweak installation of `onnxruntime-gpu` for it to work with gpu (everything will work with cpu version, just slower). For further details about recommended version refer to [this issue comment](https://github.com/microsoft/onnxruntime/issues/21684#issuecomment-2375853992) and official documentation.

### Mac MPS (Apple Silicon)

```bash
chmod +x setup_mac.sh
./setup_mac.sh
```

Or manually:

```bash
python -m venv venv_mps
source venv_mps/bin/activate
pip install -r requirements_mps.txt
```

During running all checkpoints and models are installed in local `models_cache/` directory.

## Web UI

A web-based interface for easy image generation.

### Run Backend

```bash
source venv_mps/bin/activate  # or your venv
python -m backend.main
```

Backend runs at `http://localhost:8000`

### Run Frontend

```bash
cd frontend
npm install
npm run dev
```

Frontend runs at `http://localhost:3000`

### Features

- Upload face image for identity preservation
- Upload style image for background/atmosphere transfer
- Adjust style strength (higher = more style preservation)
- Text prompt for additional control

## Evaluation dataset

We additionally release identities and prompts (realistic and stylistic) used for evaluation. Samples of identities are given in figure below.

<p align="center">
<img src="docs/dataset_samples.jpg" alt="drawing" width="500"/>
</p>

To download, activate environment created during [Installation](#installation) and run `download_data.sh` - data will be downloaded in local `data/` directory. 

## Run

For running inference on aribtrary identity/prompt pair, refer to `notebooks/inference_example.ipynb`. To run a widescale evaluation with our evaluation dataset, execute `diff_eval_idadapter.py`, refer to script for arguments details (remember to download dataset first). Basic command to run method on full data with Hyper checkpoint is given below:

```bash
python diff_eval_idadapter.py --target_adapter="faceid"\
 --ips=0.8 --lora_scale=0.8\
 --data_dir="data/"\
 --config_dir="configs/fastface/am1_and_dcg.json"\
 --ds_type="realistic"\
 --out_dir="res"\
 --exp_title="example_run"\
 --include_hyper_4\
 --device="cuda:0"
```

## Acknowledgments

This project heavily relies on source code of [diffusers](https://huggingface.co/docs/diffusers/index).

## Citation

If you find this work useful, please cite it as follows:

```bibtex
@misc{karpukhin2025fastfacetuningidentitypreservation,
      title={FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention}, 
      author={Sergey Karpukhin and Vadim Titov and Andrey Kuznetsov and Aibek Alanov},
      year={2025},
      eprint={2505.21144},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.21144}, 
}
```